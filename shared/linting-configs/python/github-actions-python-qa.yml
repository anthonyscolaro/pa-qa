# PA-QA Python Quality Assurance Pipeline
# Copy this file to .github/workflows/python-qa.yml in your project
name: Python Code Quality & Testing

on:
  push:
    branches: [ main, develop, feature/*, release/* ]
    paths:
      - '**.py'
      - '**/pyproject.toml'
      - '**/requirements*.txt'
      - '**/pytest.ini'
      - '**/ruff.toml'
      - '**/mypy.ini'
      - '.github/workflows/python-qa.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - '**.py'
      - '**/pyproject.toml'
      - '**/requirements*.txt'
      - '**/pytest.ini'
      - '**/ruff.toml'
      - '**/mypy.ini'

# Cancel running workflows when new commits are pushed
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Environment variables
env:
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1
  PIP_NO_CACHE_DIR: 1
  PIP_DISABLE_PIP_VERSION_CHECK: 1

jobs:
  # Code quality checks (fast feedback)
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            requirements*.txt
            pyproject.toml
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -e .
      
      - name: Cache pre-commit hooks
        uses: actions/cache@v3
        with:
          path: ~/.cache/pre-commit
          key: pre-commit-${{ hashFiles('.pre-commit-config.yaml') }}
      
      - name: Run Ruff linting
        run: |
          ruff check src tests --output-format=github
          ruff format --check src tests
      
      - name: Run Black formatting check
        run: |
          black --check --diff src tests
      
      - name: Run MyPy type checking
        run: |
          mypy src --junit-xml=mypy-results.xml
        continue-on-error: true
      
      - name: Upload MyPy results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: mypy-results-${{ matrix.python-version }}
          path: mypy-results.xml

  # Security scanning
  security:
    name: Security Analysis
    runs-on: ubuntu-latest
    permissions:
      security-events: write
      actions: read
      contents: read
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit[toml] safety semgrep
      
      - name: Run Bandit security scan
        run: |
          bandit -r src -f json -o bandit-results.json
        continue-on-error: true
      
      - name: Run Safety vulnerability check
        run: |
          safety check --json --output safety-results.json
        continue-on-error: true
      
      - name: Run Semgrep security analysis
        run: |
          semgrep --config=auto src --json --output=semgrep-results.json
        continue-on-error: true
      
      - name: Upload security scan results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-results
          path: |
            bandit-results.json
            safety-results.json
            semgrep-results.json

  # Comprehensive testing
  test:
    name: Tests
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.10', '3.11', '3.12']
        include:
          # FastAPI specific testing
          - framework: fastapi
            python-version: '3.11'
            os: ubuntu-latest
          # Django specific testing  
          - framework: django
            python-version: '3.11'
            os: ubuntu-latest
    
    services:
      # PostgreSQL for integration tests
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      # Redis for caching tests
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -e .
      
      - name: Install framework-specific dependencies
        if: matrix.framework
        run: |
          pip install -e ".[${{ matrix.framework }}]"
      
      - name: Set up test environment
        run: |
          export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/test_db
          export REDIS_URL=redis://localhost:6379/1
          export TESTING=true
      
      - name: Run unit tests
        run: |
          pytest tests -m "unit" \
            --cov=src \
            --cov=app \
            --cov-report=xml \
            --cov-report=html \
            --junit-xml=unit-test-results.xml \
            --tb=short \
            -v
      
      - name: Run integration tests
        run: |
          pytest tests -m "integration" \
            --cov=src \
            --cov=app \
            --cov-append \
            --cov-report=xml \
            --junit-xml=integration-test-results.xml \
            --tb=short \
            -v
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/1
      
      - name: Run E2E tests
        if: matrix.framework == 'fastapi' || matrix.framework == 'django'
        run: |
          pytest tests -m "e2e" \
            --junit-xml=e2e-test-results.xml \
            --tb=short \
            -v
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/1
      
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
          path: |
            *-test-results.xml
            htmlcov/
            coverage.xml
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  # Performance benchmarks
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -e .
      
      - name: Run benchmarks
        run: |
          pytest tests -m "benchmark" \
            --benchmark-json=benchmark-results.json \
            --benchmark-only
      
      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          comment-on-alert: true
          alert-threshold: '150%'

  # Build and package validation
  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: [quality, test]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install build dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build twine
      
      - name: Build package
        run: |
          python -m build
      
      - name: Check package
        run: |
          twine check dist/*
      
      - name: Upload build artifacts
        uses: actions/upload-artifact@v3
        with:
          name: python-package
          path: dist/

  # Documentation generation
  docs:
    name: Documentation
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
          pip install -e .
      
      - name: Build documentation
        run: |
          sphinx-build -b html docs docs/_build/html
      
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        if: github.ref == 'refs/heads/main'
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs/_build/html

  # Allure reporting integration
  allure-report:
    name: Allure Report
    runs-on: ubuntu-latest
    needs: [test]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download test artifacts
        uses: actions/download-artifact@v3
        with:
          path: test-results
      
      - name: Set up Java (for Allure)
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '11'
      
      - name: Install Allure
        run: |
          wget https://github.com/allure-framework/allure2/releases/download/2.24.0/allure-2.24.0.tgz
          tar -zxvf allure-2.24.0.tgz
          sudo mv allure-2.24.0 /opt/allure
          sudo ln -s /opt/allure/bin/allure /usr/bin/allure
      
      - name: Generate Allure report
        run: |
          allure generate test-results --output allure-report --clean
      
      - name: Upload to PA-QA Allure Dashboard
        env:
          ALLURE_ENDPOINT: https://allure.projectassistant.ai
          PROJECT_ID: ${{ github.repository }}
        run: |
          # Upload results to centralized Allure dashboard
          curl -X POST "$ALLURE_ENDPOINT/api/results" \
            -H "Content-Type: application/json" \
            -d '{
              "project": "'$PROJECT_ID'",
              "build": "'${{ github.run_number }}'",
              "branch": "'${{ github.ref_name }}'",
              "results": "allure-report"
            }'

  # Final status check
  ci-success:
    name: CI Success
    runs-on: ubuntu-latest
    needs: [quality, security, test, build]
    if: always()
    
    steps:
      - name: Check all jobs status
        run: |
          echo "Quality: ${{ needs.quality.result }}"
          echo "Security: ${{ needs.security.result }}"
          echo "Test: ${{ needs.test.result }}"
          echo "Build: ${{ needs.build.result }}"
          
          if [[ "${{ needs.quality.result }}" != "success" ]] || \
             [[ "${{ needs.test.result }}" != "success" ]] || \
             [[ "${{ needs.build.result }}" != "success" ]]; then
            echo "❌ CI pipeline failed"
            exit 1
          else
            echo "✅ CI pipeline succeeded"
          fi
      
      - name: Update commit status
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: 'success',
              description: 'PA-QA Python pipeline passed',
              context: 'PA-QA/python-qa'
            });

# Notification workflows (optional)
  notify:
    name: Notifications
    runs-on: ubuntu-latest
    needs: [ci-success]
    if: failure() && github.ref == 'refs/heads/main'
    
    steps:
      - name: Notify team of failures
        uses: actions/github-script@v6
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '🚨 **PA-QA Python Pipeline Failed**\n\nThe code quality pipeline has failed on the main branch. Please review the failed checks and fix any issues.\n\n[View Pipeline Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})'
            });